{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e21f9c-0d67-4edd-bc65-fd6a3003813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, pipeline\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04a990-25bf-43bd-bfec-85e3ba629ceb",
   "metadata": {},
   "source": [
    "# Vulnerable Identities Recognition Corpus (VIRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25d761-fab0-4f61-b18a-e4e63087d11b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7767d7-675c-476e-8b86-61179c23e3c1",
   "metadata": {},
   "source": [
    "This section contains all the methods used with the datasets. Is mandatory to run it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e5827-4c3a-4f5f-a2aa-697cccea91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def full_agreement_per_sentence(disaggregated_annotations_aux, gold_annotator_id_aux, predictions_annotator_id_aux):\n",
    "    # Number of headlines with full agreement\n",
    "    full_agreement_len_aux= {\"correct_aux\": [], \"incorrect_aux\": []}\n",
    "    for index, group in disaggregated_annotations_aux.groupby(['text_id']):\n",
    "        _tmp_agreement_recall, _ = recall(group, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "        _tmp_agreement_precision, _ = precision(group, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "        _tmp_f1_scores = calculate_f1(_tmp_agreement_recall, _tmp_agreement_precision)\n",
    "        is_correct= True # Assume correct label\n",
    "        for results in _tmp_f1_scores:\n",
    "            if results[\"f1_score\"] <= 0:\n",
    "                is_correct= False\n",
    "                break\n",
    "        if is_correct: \n",
    "            full_agreement_len_aux[\"correct_aux\"].append(index)\n",
    "        else: \n",
    "            full_agreement_len_aux[\"incorrect_aux\"].append(index)\n",
    "\n",
    "    # Count the headlines\n",
    "    full_agreement_len_aux = {\n",
    "            'correct': len(full_agreement_len_aux['correct_aux']),\n",
    "            'incorrect': len(full_agreement_len_aux['incorrect_aux'])\n",
    "    }\n",
    "\n",
    "    #full_agreement_len_aux= pd.DataFrame.from_dict(full_agreement_len_aux, orient=\"index\")\n",
    "    return full_agreement_len_aux\n",
    "\n",
    "def full_agreement_per_annotation(disaggregated_annotations_aux, gold_annotator_id_aux, predictions_annotator_id_aux):\n",
    "    # Number of headlines with agreement per label\n",
    "\n",
    "    agreement_label_aux= {}\n",
    "    for index, group in disaggregated_annotations_aux.groupby(['text_id', 'label']):\n",
    "        _tmp_agreement_recall, _ = recall(group, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "        _tmp_agreement_precision, _ = precision(group, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "        _tmp_f1_scores = calculate_f1(_tmp_agreement_recall, _tmp_agreement_precision)\n",
    "        for results in _tmp_f1_scores:\n",
    "            if results[\"label\"] not in agreement_label_aux: agreement_label_aux[results[\"label\"]]= {\"correct_aux\": [], \"incorrect_aux\": []}\n",
    "\n",
    "            if results[\"f1_score\"] > 0 and index[0] not in agreement_label_aux[results[\"label\"]][\"correct_aux\"]:\n",
    "                agreement_label_aux[results[\"label\"]][\"correct_aux\"].append(index[0])\n",
    "\n",
    "            if results[\"f1_score\"] <= 0 and index[0] not in agreement_label_aux[results[\"label\"]][\"incorrect_aux\"]:\n",
    "                agreement_label_aux[results[\"label\"]][\"incorrect_aux\"].append(index[0])\n",
    "\n",
    "    # Count the headlines\n",
    "    agreement_label_len_aux = {\n",
    "        label: {\n",
    "            'correct': len(data['correct_aux']),\n",
    "            'incorrect': len(data['incorrect_aux'])\n",
    "        }\n",
    "        for label, data in agreement_label_aux.items()\n",
    "    }\n",
    "\n",
    "    agreement_label_len_aux= pd.DataFrame.from_dict(agreement_label_len_aux, orient=\"index\").sort_index()\n",
    "    return agreement_label_len_aux\n",
    "\n",
    "def f_scores_per_label(disaggregated_annotations_aux, gold_annotator_id_aux, predictions_annotator_id_aux):\n",
    "    # Calculate metrics\n",
    "    agreement_recall_aux, _ = recall(disaggregated_annotations_aux, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "    agreement_precision_aux, _ = precision(disaggregated_annotations_aux, gold_annotator_id_aux, predictions_annotator_id_aux)\n",
    "    f1_scores_aux = calculate_f1(agreement_recall_aux, agreement_precision_aux)\n",
    "\n",
    "    df= disaggregated_annotations_aux # For having shorter code\n",
    "    for performance in f1_scores_aux:\n",
    "            performance[\"n_annotations\"]  = len(df[ (df[\"label\"]==performance[\"label\"]) & (df[\"annotator_id\"]==gold_annotator_id_aux) ])\n",
    "            performance[\"n_annotations\"] += len(df[ (df[\"label\"]==performance[\"label\"]) & (df[\"annotator_id\"]==predictions_annotator_id_aux) ])\n",
    "\n",
    "    f1_scores_aux= pd.DataFrame(f1_scores_aux).sort_values('label', ascending=True, ignore_index=True)\n",
    "    return f1_scores_aux\n",
    "\n",
    "def extract_annotations(path): # Method to get formatted annotations from a json path\n",
    "    all_annotations = list()\n",
    "    with open(path) as f:\n",
    "        jsn = json.load(f)\n",
    "    \n",
    "    for item in jsn['tasks']:\n",
    "        for el in item['task']['text_lines']:\n",
    "            if 'annotations' in el:\n",
    "                text = el['data']['text']\n",
    "                _id = el['text_id'] \n",
    "                for annotations in el['annotations']:\n",
    "                    for annotation in annotations:\n",
    "                        d = dict()\n",
    "                        d['text_id'] = _id\n",
    "                        d['text'] = text\n",
    "                        d['annotator_id'] = annotation['annotator_id']\n",
    "                        d['purpose'] = annotation['value']['purpose']\n",
    "                        d['label'] = annotation['value']['text'] if d['purpose'] == 'tagging' else None\n",
    "                        d['comment'] = annotation['value']['text'] if d['purpose'] == 'commenting' else None\n",
    "                        d['span'] = annotation['value']['interval']['exact_highlight']\n",
    "                        d['start_idx'] = annotation['value']['interval']['start_at']\n",
    "                        d['end_idx'] = annotation['value']['interval']['end_at']\n",
    "                        d['text_external_id'] = el['text_external_id']\n",
    "                        all_annotations.append(d)\n",
    "    return all_annotations\n",
    "\n",
    "def spans_fully_contain(span1, span2):\n",
    "    \"\"\"\n",
    "    Check if one span fully contains the other span in either direction.\n",
    "\n",
    "    Parameters:\n",
    "    span1 (tuple): A tuple containing the start and end of the first span (start1, end1).\n",
    "    span2 (tuple): A tuple containing the start and end of the second span (start2, end2).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if either span fully contains the other, False otherwise.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1\n",
    "    start2, end2 = span2\n",
    "    return (start1 <= start2 and end1 >= end2) or (start2 <= start1 and end2 >= end1)\n",
    "\n",
    "def spans_overlap(span1, span2):\n",
    "    \"\"\"Check if two spans overlap.\n",
    "    \n",
    "    Parameters:\n",
    "    span1 (tuple): A tuple containing the start and end of the first span (start1, end1).\n",
    "    span2 (tuple): A tuple containing the start and end of the second span (start2, end2).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if either span overlap the other, False otherwise.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1\n",
    "    start2, end2 = span2\n",
    "    return max(start1, start2) <= min(end1, end2)  # Checks if there's any overlap\n",
    "\n",
    "\n",
    "def recall(df, gold_annotator_id, predictions_annotator_id, generate_file=False):\n",
    "    df = df[df.label.notna()]\n",
    "    annotators = df.annotator_id.drop_duplicates().values\n",
    "    results = list()\n",
    "    report = list()\n",
    "    for label in df.label.drop_duplicates().values:\n",
    "        recall = list()\n",
    "        tmp = df[df.label==label]\n",
    "        n_labels = len(tmp[['label','text_id']].drop_duplicates())\n",
    "        tmp = tmp.groupby(['text_id','annotator_id', 'text_external_id', 'start_idx', 'end_idx']).span.apply(list).reset_index()\n",
    "        a_id = gold_annotator_id\n",
    "        b_id = predictions_annotator_id\n",
    "        for _, a_annotation in tmp[tmp.annotator_id==a_id].iterrows():\n",
    "            # For each annotation of the first annotator\n",
    "            mtc = 0 \n",
    "\n",
    "            b_annotations = tmp[(tmp.annotator_id==b_id)&(tmp.text_id==a_annotation.text_id)] # Get the second annotator annotations for the same sentence\n",
    "            for _, b_annotation in b_annotations.iterrows():\n",
    "                if spans_fully_contain((a_annotation.start_idx, a_annotation.end_idx), (b_annotation.start_idx, b_annotation.end_idx)):\n",
    "                    # Match, one span contains the other\n",
    "                    mtc= 1 # True positive\n",
    "                    continue\n",
    "                elif spans_overlap((a_annotation.start_idx, a_annotation.end_idx), (b_annotation.start_idx, b_annotation.end_idx)): \n",
    "                    # Partial match\n",
    "                    mtc = 0.5 # Partial true positive\n",
    "                    continue\n",
    "\n",
    "            if generate_file is True and mtc==0:\n",
    "                report.append({'text_id':a_annotation.text_id,'label':label, 'text_external_id': a_annotation.text_external_id})\n",
    "                \n",
    "            recall.append(mtc)\n",
    "                \n",
    "        recall = Counter(recall)\n",
    "\n",
    "        if recall[1] > 0:\n",
    "            results.append({'label':label,'annotator_1':a_id,'annotator_2':b_id,'recall':recall[1]/(recall[1]+recall[0]),'support':recall[1]+recall[0]})\n",
    "        else:\n",
    "            results.append({'label':label,'annotator_1':a_id,'annotator_2':b_id,'recall':0,'support':recall[1]+recall[0]})\n",
    "\n",
    "    return results,report\n",
    "\n",
    "def precision(df, gold_annotator_id, predictions_annotator_id, generate_file=False):\n",
    "    df = df[df.label.notna()]\n",
    "    annotators = df.annotator_id.drop_duplicates().values\n",
    "    results = list()\n",
    "    report = list()\n",
    "    for label in df.label.drop_duplicates().values:\n",
    "        precision = list()\n",
    "        tmp = df[df.label==label]\n",
    "        n_labels = len(tmp[['label','text_id']].drop_duplicates())\n",
    "        tmp = tmp.groupby(['text_id','annotator_id','text_external_id', 'start_idx', 'end_idx']).span.apply(list).reset_index()\n",
    "        a_id = gold_annotator_id\n",
    "        b_id = predictions_annotator_id\n",
    "        for _, b_annotation in tmp[tmp.annotator_id==b_id].iterrows():\n",
    "            mtc = 0 # If mtc is 0 is a false positive\n",
    "            a_annotations = tmp[(tmp.annotator_id==a_id)&(tmp.text_id==b_annotation.text_id)] # Get the second annotator annotations for the same sentence\n",
    "            for _, a_annotation in a_annotations.iterrows():\n",
    "                if spans_fully_contain((a_annotation.start_idx, a_annotation.end_idx), (b_annotation.start_idx, b_annotation.end_idx)):\n",
    "                    # Match, one span contains the other\n",
    "                    mtc= 1 # True positive\n",
    "                    continue\n",
    "                elif spans_overlap((a_annotation.start_idx, a_annotation.end_idx), (b_annotation.start_idx, b_annotation.end_idx)):\n",
    "                    # Partial match\n",
    "                    mtc = 0.5 # True positive\n",
    "                    continue\n",
    "            if generate_file is True and mtc==0:\n",
    "                report.append({'text_id':a_annotation.text_id,'label':label, 'text_external_id': a_annotation.text_external_id})\n",
    "            \n",
    "            precision.append(mtc)\n",
    "            \n",
    "        precision = Counter(precision)\n",
    "        if precision[1] >0:\n",
    "            results.append({'label':label,'annotator_1':a_id,'annotator_2':b_id,'precision':precision[1]/(precision[1]+precision[0]),'support':precision[1]+precision[0]})\n",
    "        else:\n",
    "            results.append({'label':label,'annotator_1':a_id,'annotator_2':b_id,'precision':0,'support':precision[1]+precision[0]})\n",
    "    return results, report\n",
    "\n",
    "def calculate_f1(agreement_recall, agreement_precision):\n",
    "    f1_scores = []\n",
    "    for recall_entry, precision_entry in zip(agreement_recall, agreement_precision):\n",
    "        label = recall_entry['label']\n",
    "        recall_value = recall_entry['recall']\n",
    "        precision_value = precision_entry['precision']\n",
    "        \n",
    "        f1_value = f1(precision_value, recall_value)\n",
    "        \n",
    "        f1_scores.append({'label': label, 'f1_score': f1_value, 'recall': recall_value, 'precision': precision_value})\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "def f1(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        # If both recall and precision are 0, F1 should also be 0\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def majority_vote(annotations, final_annotator_id=None):\n",
    "    \"\"\"\n",
    "    Returns a list of target spans that have majority agreement among the annotators.\n",
    "\n",
    "    Parameters:\n",
    "    - annotations: List of lists, where each inner list contains tuples representing \n",
    "                   (start_idx, end_idx) of annotation spans for each annotator for a specific sentence.\n",
    "\n",
    "    - final_annotator_id: Id of the resulting annotation annotator. Default None.\n",
    "    Returns:\n",
    "    - List of tuples: Target spans that received majority agreement.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to hold the counts of votes for each span\n",
    "    span_votes = {}\n",
    "\n",
    "    all_data= {}\n",
    "    strict_majority = True # To only take into account labels with majority vote\n",
    "    on_overlap_mantain = max # Max for the biggest span, min for the smallest\n",
    "\n",
    "    # Iterate over each annotator's spans\n",
    "    for annotator_index_a, annotations_a in enumerate(annotations): # For each annotator\n",
    "        for annotation_index_a, annotation_a in enumerate(annotations_a): # For each annotation\n",
    "            \n",
    "            idx_with_label_a = (annotation_a[\"start_idx\"], annotation_a[\"end_idx\"]), annotation_a[\"label\"]\n",
    "            all_data[idx_with_label_a]= annotation_a\n",
    "\n",
    "            # Add the span itself to the votes\n",
    "            if idx_with_label_a not in span_votes:\n",
    "                span_votes[idx_with_label_a] = 0\n",
    "            span_votes[idx_with_label_a] += 1\n",
    "\n",
    "            overlapping_spans = []\n",
    "\n",
    "            for annotator_index_b, annotator_spans_b in enumerate(annotations): # For each annotator\n",
    "                for annotation_index_b, annotation_b in enumerate(annotator_spans_b): # For each annotation\n",
    "                    if annotator_index_a == annotator_index_b and \\\n",
    "                        annotation_index_a == annotation_index_b: continue # omit the current annotation when overlap check\n",
    "                    \n",
    "                    idx_with_label_b= (annotation_b[\"start_idx\"], annotation_b[\"end_idx\"]), annotation_b[\"label\"]\n",
    "\n",
    "                    # If same label annotated\n",
    "                    # if spans overlap\n",
    "                    # if not previously counted\n",
    "                    if  annotation_a[\"label\"] == annotation_b[\"label\"] and \\\n",
    "                        spans_overlap((annotation_a[\"start_idx\"], annotation_a[\"end_idx\"]), (annotation_b[\"start_idx\"], annotation_b[\"end_idx\"])) \\\n",
    "                        and idx_with_label_b not in overlapping_spans:\n",
    "                        \n",
    "                        # overlapping_spans.append(idx_with_label_b)\n",
    "                        overlapping_spans.append(idx_with_label_a)\n",
    "            \n",
    "            if overlapping_spans: # If overlaps exist\n",
    "                for existing_span in overlapping_spans: # For each overlap vote\n",
    "                    if existing_span not in span_votes:\n",
    "                        span_votes[existing_span] = 0\n",
    "                    span_votes[existing_span] += 1 \n",
    "    \n",
    "    # del counted_overlaps # Delete aux array\n",
    "    # Determine majority agreement spans\n",
    "    majority_spans = {}\n",
    "    total_annotators = len(annotations)\n",
    "\n",
    "    for ((start_idx, end_idx), label), votes in span_votes.items():\n",
    "        span_a= (start_idx, end_idx)\n",
    "        if votes > (total_annotators / 2):  # More than half for majority\n",
    "            if label not in majority_spans:\n",
    "                majority_spans[label]= [(span_a, label)]\n",
    "            else: # if label already exist can be two cases. Multiple different spans or overlapped span.\n",
    "                is_overlapped= False\n",
    "                for idx_majority, (span_b, label_b) in enumerate(majority_spans[label]):\n",
    "                    if spans_overlap(span_a, span_b):\n",
    "                        is_overlapped= True\n",
    "                        majority_spans[label][idx_majority]= (on_overlap_mantain([span_a, span_b], key=lambda span: span[1] - span[0]), label_b)  # Sort by length of the span)\n",
    "\n",
    "                if not is_overlapped: # If is a new label\n",
    "                    majority_spans[label].append((span_a, label))\n",
    "        else:\n",
    "            if not strict_majority:\n",
    "                # If not majority vote we can also include it.\n",
    "                if label not in majority_spans:\n",
    "                    majority_spans[label]= [(span_a, label)]\n",
    "\n",
    "    definitive= []\n",
    "    for _label, _annotations in majority_spans.items():\n",
    "        for _annotation in _annotations:\n",
    "            if final_annotator_id is not None: all_data[_annotation][\"annotator_id\"]= final_annotator_id\n",
    "            definitive.append(all_data[_annotation])\n",
    "    return definitive # Return an empty list if no majority found\n",
    "\n",
    "def create_gold_standard(df, final_annotator_id= \"gold_standard\"):\n",
    "    \"\"\"\n",
    "    Create a gold standard corpus using majority voting.\n",
    "    \"\"\"\n",
    "    gold_standard = []\n",
    "\n",
    "    # Group annotations by text_id, start_idx, end_idx\n",
    "    grouped = df.groupby(['text_id', 'label'])\n",
    "    label_performance= {}\n",
    "    \n",
    "    annotator_ids= df.annotator_id.unique()\n",
    "    for text_id, group in grouped:\n",
    "        # Collect all spans for the same annotator\n",
    "        annotation_spans = []\n",
    "        for annotator_id in annotator_ids:\n",
    "            _annotations= []\n",
    "            for n_annotations in range(len(group[group.annotator_id == annotator_id])):\n",
    "                _annotations.append(group[group.annotator_id == annotator_id].iloc[n_annotations].to_dict())\n",
    "            annotation_spans.append(_annotations)\n",
    "        \n",
    "        # Check if there's a majority vote for this span\n",
    "        gold_annotations= majority_vote(annotation_spans, final_annotator_id=final_annotator_id)\n",
    "        gold_standard.extend(gold_annotations)\n",
    "\n",
    "    return pd.DataFrame(gold_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e272d8-65f9-4f68-b8f1-8113f29497df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a775062-fbeb-439b-9822-ebe4df21dd7a",
   "metadata": {},
   "source": [
    "- **Disaggregated dataset**: disaggregated_annotations_ita and disaggregated_annotations_spa dataframes store the disaggregated annotations. \n",
    "- **Aggregated dataset**: ita_gold and spa_gold dataframes store the aggregated annotations. \n",
    "\n",
    "The dataset structure is as follows:\n",
    "| text_id | text | annotator_id | purpose | label | comment | span | start_idx | end_idx | text_external_id |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442304a-49d5-464e-b3bb-d223812ba84e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Disaggregated Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53383485-bedf-4dde-b65b-4950c6ffcb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_annotations_files= [\"Data/annotations_italian_1.json\",\n",
    "                        \"Data/annotations_italian_2.json\"]\n",
    "\n",
    "spa_annotations_files= [\"Data/annotations_spanish_1.json\",\n",
    "                        \"Data/annotations_spanish_2.json\"]\n",
    "\n",
    "disaggreement_annotations_files_spa= [\"Data/annotations_spanish_disagreement.json\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dfcd7-bcd2-4061-8a3d-35bacf411325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract annotations from italian subsets\n",
    "disaggregated_annotations_ita = []\n",
    "for p in ita_annotations_files:\n",
    "    disaggregated_annotations_ita.extend(extract_annotations(p))\n",
    "\n",
    "disaggregated_annotations_ita = pd.DataFrame(disaggregated_annotations_ita)\n",
    "disaggregated_annotations_ita.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b364ba-a46c-4af2-9670-fcc8d87bd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract annotations from spanish subsets\n",
    "disaggregated_annotations_spa = []\n",
    "for p in spa_annotations_files:\n",
    "    disaggregated_annotations_spa.extend(extract_annotations(p))\n",
    "\n",
    "disaggregated_annotations_spa = pd.DataFrame(disaggregated_annotations_spa)\n",
    "disaggregated_annotations_spa.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa93661-86d4-4337-aab4-ab4a361c2d21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b84ae5-fac9-4e3d-986a-d930c7fcd8c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86008c53-087e-483d-a1d9-692bd2c93cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian annotator ids: 7, 9\n",
    "gold_annotator_id_ita= 7\n",
    "predictions_annotator_id_ita= 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea72975-e9af-4753-b773-1cd909d3f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_f1_scores_ita= f_scores_per_label(disaggregated_annotations_ita, gold_annotator_id_ita, predictions_annotator_id_ita)\n",
    "print(\"Italian f1 scores per label\")\n",
    "agreement_f1_scores_ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccf851-d865-4337-9a19-6d6f9f084686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full agreement stats\n",
    "agreement_in_n_sentences_ita= full_agreement_per_sentence(disaggregated_annotations_ita, gold_annotator_id_ita, predictions_annotator_id_ita)\n",
    "print(f\"From the {agreement_in_n_sentences_ita['correct']+agreement_in_n_sentences_ita['incorrect']} sentences in the Italian dataset, only {agreement_in_n_sentences_ita['correct']} have a high agreement (above 0.5 agreement). This means that {agreement_in_n_sentences_ita['incorrect']} have no full agreement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca928a26-01f0-490e-b242-91ddc9e06848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations agreement stats\n",
    "agreement_in_annotations_ita= full_agreement_per_annotation(disaggregated_annotations_ita, gold_annotator_id_ita, predictions_annotator_id_ita)\n",
    "print(\"The agreement between annotator for each label. This shows the number of annotations that have a high annotation (above 0.5 agreement) for each label in each sentence\")\n",
    "agreement_in_annotations_ita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59312efd-bf92-4dbb-8bcd-bcd2b5893e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c9152-9817-4a9b-963b-534fe232c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish annotator ids: 7, 27\n",
    "gold_annotator_id_spa= 27\n",
    "predictions_annotator_id_spa= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952d4d4-09bd-45e3-ada8-7da0f191bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_f1_scores_spa= f_scores_per_label(disaggregated_annotations_spa, gold_annotator_id_spa, predictions_annotator_id_spa)\n",
    "print(\"Spanish f1 scores per label\")\n",
    "agreement_f1_scores_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1e92e-f541-478d-aff5-d36900d380ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full agreement stats\n",
    "agreement_in_n_sentences_spa= full_agreement_per_sentence(disaggregated_annotations_spa, gold_annotator_id_spa, predictions_annotator_id_spa)\n",
    "print(f\"From the {agreement_in_n_sentences_spa['correct']+agreement_in_n_sentences_spa['incorrect']} sentences in the Spanish dataset, only {agreement_in_n_sentences_spa['correct']} have a high agreement (above 0.5 agreement). This means that {agreement_in_n_sentences_spa['incorrect']} have no full agreement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a6d11-e952-4ba3-b701-453c7d180c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations agreement stats\n",
    "agreement_in_annotations_spa= full_agreement_per_annotation(disaggregated_annotations_spa, gold_annotator_id_spa, predictions_annotator_id_spa)\n",
    "print(\"The agreement between annotator for each label. This shows the number of annotations that have a high annotation (above 0.5 agreement) for each label in each sentence\")\n",
    "agreement_in_annotations_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1d257-5b61-42f0-877c-b5650fd0c8be",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stats and Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5617da-f55d-4dc7-b913-6e6e077369a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 scores per label in both datasets and the number of annotations\n",
    "\n",
    "_labels= sorted(list(set(agreement_f1_scores_ita[\"label\"].unique().tolist() + agreement_f1_scores_spa[\"label\"].unique().tolist())))\n",
    "_f1_spanish= [ round(agreement_f1_scores_spa[agreement_f1_scores_spa[\"label\"]==label][\"f1_score\"].iloc[0], 2) if label in agreement_f1_scores_spa[\"label\"].tolist() else \"-\" for label in _labels]\n",
    "_n_ann_spanish= [ agreement_f1_scores_spa[agreement_f1_scores_spa[\"label\"]==label][\"n_annotations\"].iloc[0] if label in agreement_f1_scores_spa[\"label\"].tolist() else \"-\" for label in _labels]\n",
    "_f1_italian= [ round(agreement_f1_scores_ita[agreement_f1_scores_ita[\"label\"]==label][\"f1_score\"].iloc[0], 2) if label in agreement_f1_scores_ita[\"label\"].tolist() else \"-\" for label in _labels]\n",
    "_n_ann_italian= [ agreement_f1_scores_ita[agreement_f1_scores_ita[\"label\"]==label][\"n_annotations\"].iloc[0] if label in agreement_f1_scores_ita[\"label\"].tolist() else \"-\" for label in _labels]\n",
    "\n",
    "agreement_f1_scores_aggregated= pd.DataFrame({\n",
    "    \"labels\": _labels,\n",
    "    \"Spanish F1\": _f1_spanish,\n",
    "    \"Italian F1\": _f1_italian,\n",
    "    \"Spanish n_ann\": _n_ann_spanish,\n",
    "    \"Italian n_ann\": _n_ann_italian\n",
    "})\n",
    "\n",
    "# print(agreement_f1_scores_aggregated.drop(columns=[\"Italian n_ann\", \"Spanish n_ann\"]).to_latex(index=False)) # For replicating the paper table\n",
    "agreement_f1_scores_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adf332-69fb-4596-a975-890124633e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of headlines with an acceptable f1 score (above 0.5) per label\n",
    "merged_df = pd.merge(agreement_in_annotations_spa, agreement_in_annotations_ita, left_index=True, right_index=True, how='outer', suffixes=('_spa', '_ita'))\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3b7b1-e1c2-4001-9075-ad25184e4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of annotations per label category for both Italian and Spanish datasets\n",
    "\n",
    "# Convert 'Spanish n_ann' and 'Italian n_ann' to numeric, coercing errors to NaN (for the '-' entry)\n",
    "agreement_f1_scores_aggregated['Spanish n_ann'] = pd.to_numeric(agreement_f1_scores_aggregated['Spanish n_ann'], errors='coerce')\n",
    "agreement_f1_scores_aggregated['Italian n_ann'] = pd.to_numeric(agreement_f1_scores_aggregated['Italian n_ann'], errors='coerce')\n",
    "\n",
    "# Target labels\n",
    "_labels_grouped= [\"dangerous\", \"derogatory\", \"entities\", \"vulnerable identities\" ]\n",
    "\n",
    "# Spanish\n",
    "_spa_grp__vg= agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'].str.startswith('vulnerable group')]['Spanish n_ann'].sum() + \\\n",
    "         agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels']== \"vulnerable identity\"]['Spanish n_ann'].values[0]\n",
    "\n",
    "_n_ann_spanish_grouped= [agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'] == \"dangerous\"][\"Spanish n_ann\"].sum(),\n",
    "                         agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'] == \"derogatory\"][\"Spanish n_ann\"].sum(),\n",
    "                         agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'].str.startswith('entity')]['Spanish n_ann'].sum(), \n",
    "                         _spa_grp__vg]\n",
    "\n",
    "# Italian\n",
    "_ita_grp__vg = agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'].str.startswith('vulnerable group')]['Italian n_ann'].sum() + \\\n",
    "               agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels']== \"vulnerable identity\"]['Italian n_ann'].values[0]\n",
    "\n",
    "_n_ann_italian_grouped= [agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'] == \"dangerous\"][\"Italian n_ann\"].sum(),\n",
    "                         agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'] == \"derogatory\"][\"Italian n_ann\"].sum(),\n",
    "                         agreement_f1_scores_aggregated[agreement_f1_scores_aggregated['labels'].str.startswith('entity')]['Italian n_ann'].sum(),\n",
    "                         _ita_grp__vg]\n",
    "\n",
    "n_annotations_grouped= pd.DataFrame({\"labels\": _labels_grouped, \"Spanish n_ann\": _n_ann_spanish_grouped,\"Italian n_ann\": _n_ann_italian_grouped})\n",
    "\n",
    "print(\"Label distribution of the disaggregated dataset grouped in categories\")\n",
    "print(\"---\"*10)\n",
    "# print(n_annotations_grouped.to_latex(index=False)) # For replicating the paper table\n",
    "n_annotations_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ba0a9-a200-4cef-adc6-7bdf7c7eaffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gold-Standard Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dda24-4270-4589-bad6-5628ab2ccfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file_path_ita= \"Data/ita_gold.csv\"\n",
    "gold_file_path_spa= \"Data/spa_gold.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e20d22-ce25-4381-a72b-a2d75f2158d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa8a1c-a564-4321-af4f-59411aff4b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Italian gold-standard dataset\n",
    "ita_gold= create_gold_standard(disaggregated_annotations_ita)\n",
    "ita_gold.to_csv(gold_file_path_ita, index=False)\n",
    "ita_gold.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbeaf2-7193-4088-8c2e-cbfaa4cd4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the disaggreement annotation\n",
    "spa_gold= disaggregated_annotations_spa.copy()\n",
    "for ann in disaggreement_annotations_files_spa:\n",
    "    pd.concat([spa_gold, pd.DataFrame(extract_annotations(ann))])\n",
    "\n",
    "# Generate gold-standard dataset\n",
    "spa_gold= create_gold_standard(spa_gold)\n",
    "spa_gold.to_csv(gold_file_path_spa, index=False)\n",
    "spa_gold.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9987d-85e1-4da0-bf00-4fdd1571c8ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75d767-1e73-4d17-820e-2399f5c81d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Italian gold-standard dataset\n",
    "ita_gold= pd.read_csv(gold_file_path_ita)\n",
    "\n",
    "# Load Spanish gold-standard dataset\n",
    "spa_gold= pd.read_csv(gold_file_path_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b658a-1fd2-4511-b0f9-c6f3c51c835c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stats and Label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b18461-a7ab-44eb-8d3e-c333044be5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of annotations per label category for both Italian and Spanish datasets\n",
    "\n",
    "_labels= sorted(list(set(spa_gold[\"label\"].unique().tolist() + ita_gold[\"label\"].unique().tolist())))\n",
    "\n",
    "spa_gold_n_ann_aggregated= [len(spa_gold[(spa_gold[\"label\"]==label)]) if label in list(spa_gold[\"label\"]) else \"-\" for label in _labels]\n",
    "ita_gold_n_ann_aggregated= [len(ita_gold[(ita_gold[\"label\"]==label)]) if label in list(ita_gold[\"label\"]) else \"-\" for label in _labels]\n",
    "\n",
    "gold_aggregated= pd.DataFrame({\"labels\": _labels, \n",
    "                              \"Spanish n_ann\": spa_gold_n_ann_aggregated,\n",
    "                              \"Italian n_ann\": ita_gold_n_ann_aggregated})\n",
    "\n",
    "# Convert 'Spanish n_ann' and 'Italian n_ann' to numeric, coercing errors to NaN (for the '-' entry)\n",
    "gold_aggregated['Spanish n_ann'] = pd.to_numeric(gold_aggregated['Spanish n_ann'], errors='coerce')\n",
    "gold_aggregated['Italian n_ann'] = pd.to_numeric(gold_aggregated['Italian n_ann'], errors='coerce')\n",
    "\n",
    "# Target labels\n",
    "_labels_grouped= [\"dangerous\", \"derogatory\", \"entities\", \"vulnerable identities\" ]\n",
    "\n",
    "# Spanish\n",
    "_spa_grp__vg= gold_aggregated[gold_aggregated['labels'].str.startswith('vulnerable group')]['Spanish n_ann'].sum()\n",
    "\n",
    "_n_ann_spanish_grouped= [gold_aggregated[gold_aggregated['labels'] == \"dangerous\"][\"Spanish n_ann\"].sum(),\n",
    "                         gold_aggregated[gold_aggregated['labels'] == \"derogatory\"][\"Spanish n_ann\"].sum(),\n",
    "                         gold_aggregated[gold_aggregated['labels'].str.startswith('entity')]['Spanish n_ann'].sum(), \n",
    "                         _spa_grp__vg]\n",
    "\n",
    "# Italian\n",
    "_ita_grp__vg = gold_aggregated[gold_aggregated['labels'].str.startswith('vulnerable group')]['Italian n_ann'].sum()\n",
    "\n",
    "_n_ann_italian_grouped= [gold_aggregated[gold_aggregated['labels'] == \"dangerous\"][\"Italian n_ann\"].sum(),\n",
    "                         gold_aggregated[gold_aggregated['labels'] == \"derogatory\"][\"Italian n_ann\"].sum(),\n",
    "                         gold_aggregated[gold_aggregated['labels'].str.startswith('entity')]['Italian n_ann'].sum(),\n",
    "                         _ita_grp__vg]\n",
    "\n",
    "gold_gruped= pd.DataFrame({\"labels\": _labels_grouped, \"Spanish n_ann\": _n_ann_spanish_grouped,\"Italian n_ann\": _n_ann_italian_grouped})\n",
    " \n",
    "print(\"Label distribution of the aggregated dataset\")\n",
    "print(\"---\"*10)\n",
    "# print(gold_gruped.to_latex(index=False)) # For replicating the paper table\n",
    "\n",
    "gold_gruped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29279e15-22f0-4779-800c-116cf5683965",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test LLM Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9340d4e-945f-4d51-966c-449733b6f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "improtant_labels= [\"dangerous\", \"derogatory\", \"entity\", \"vulnerable identity\"] # Labels that are going to be shown (to reduce noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd74e668-aa71-4b57-9365-3e60b208eb3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiments definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de70a1-a7a2-40e2-b7d0-4fba0d7510b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset adaptation. In the paper we reduce the work to four label categories: dangerous, derogatory, vulnerable identities and named entities\n",
    "category_mapper= {\n",
    "                'dangerous': \"dangerous\",\n",
    "                'derogatory': \"derogatory\",\n",
    "\n",
    "                'entity': \"entity\",\n",
    "                'entity - person': \"entity\",\n",
    "                'entity - group': \"entity\",\n",
    "                'entity - organization': \"entity\",\n",
    "                'entity - location': \"entity\",\n",
    "                'entity - other': \"entity\",\n",
    "\n",
    "                'vulnerable group - migrant': 'vulnerable identity',\n",
    "                'vulnerable group - ethnic minority': 'vulnerable identity',\n",
    "                'vulnerable group - religious minority': 'vulnerable identity',\n",
    "                'vulnerable group - women': 'vulnerable identity',\n",
    "                'vulnerable group - lgbtq+ community': 'vulnerable identity',\n",
    "                'vulnerable group - other': 'vulnerable identity',\n",
    "                'vulnerable identity': 'vulnerable identity'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3d258-90fc-43b0-a3ca-ded4650b12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_restrictive_zero_shot(df, model, model_name, prompts, task_name= \"restrictive-zero-shot\"):\n",
    "    # Approach 1: restrictive-zero-shot \n",
    "    # The model knows which and how many tags are present in the sentence. It has to find the span corresponding to each one. Given a sentence and the tag, it finds the span.\n",
    "\n",
    "    model_generated= []\n",
    "    existing_answers= {}\n",
    "    annotation_options= 5\n",
    "    \n",
    "    for annotations_index, annotations in tqdm(df.groupby([\"text_id\"])):\n",
    "        if annotations_index not in existing_answers:\n",
    "            existing_answers[annotations_index]= []\n",
    "        for annotation_index, annotation in annotations.iterrows():\n",
    "            \n",
    "            prompt= {\"question\" : prompts[annotation.label],\n",
    "                     \"context\" : annotation.text}\n",
    "            \n",
    "            answers= model(**prompt, top_k=annotation_options)\n",
    "            \n",
    "            for posible_anser_idx in range(annotation_options):\n",
    "                answer= answers[posible_anser_idx]\n",
    "                \n",
    "                if ((answer[\"start\"], answer[\"end\"]), answer[\"answer\"]) not in existing_answers[annotations_index]:\n",
    "                    existing_answers[annotations_index].append(((answer[\"start\"], answer[\"end\"]), answer[\"answer\"]))\n",
    "                \n",
    "                    _annotation= annotation.copy()\n",
    "                    _annotation['annotator_id']= f\"{task_name}/{model_name}\"\n",
    "                    _annotation['span']= answer[\"answer\"]\n",
    "                    _annotation['start_idx']= answer[\"start\"]\n",
    "                    _annotation['end_idx']= answer[\"end\"]\n",
    "                    _annotation['comment']= None\n",
    "                    \n",
    "                    model_generated.append(_annotation)\n",
    "                    break\n",
    "    return pd.DataFrame(model_generated)\n",
    "\n",
    "def experiment_non_restrictive_zero_shot(df, model, model_name, prompts, task_name= \"non-restrictive-zero-shot\", score_treshold=0.002):\n",
    "    # Approach 2: non-restrictive zero-shot\n",
    "    # The model does not know how many or which tags are present in the text. It finds the span and the tags present. It is passed a sentence, finds tags and span.    \n",
    "    \n",
    "    annotation_max= 3 # number of annotations extracted per query to the model. Each annotation will be asumed to be from a different annotator to later create a majority vote.\n",
    "    model_generated= []\n",
    "\n",
    "    for annotations_index, annotations in tqdm(df.groupby([\"text_id\", \"text\"])):\n",
    "\n",
    "        for label, question in prompts.items():\n",
    "            prompt= {\"question\" : prompts[label],\n",
    "                     \"context\" : annotations.iloc[0].text}\n",
    "\n",
    "            answers= model(**prompt, top_k=annotation_max)\n",
    "\n",
    "            for idx_answer, answer in enumerate(answers):\n",
    "                if answer[\"score\"] <= score_treshold: continue\n",
    "                to_add= annotations.iloc[0].copy()\n",
    "                to_add.annotator_id = f\"{task_name}/{model_name}/{idx_answer}\"\n",
    "                to_add.label = label\n",
    "                to_add.span = answer[\"answer\"]\n",
    "                to_add.start_idx = answer[\"start\"]\n",
    "                to_add.end_idx = answer[\"end\"]\n",
    "                if to_add.start_idx == 0 and to_add.end_idx == len(to_add.text):\n",
    "                    continue # If full sentence is passed is not valid annotation\n",
    "                model_generated.append(to_add)\n",
    "\n",
    "    model_generated= pd.DataFrame(model_generated)\n",
    "    model_generated= create_gold_standard(model_generated, final_annotator_id=f\"{task_name}/{model_name}\") # Aggregate all the annotations by majority vote\n",
    "\n",
    "    return model_generated\n",
    "\n",
    "def eval_experiments(df, model_name, task_name, gold_annotator_id= \"gold_standard\"):\n",
    "    predictions_annotator_id= f\"{task_name}/{model_name}\"\n",
    "\n",
    "    agreement_recall, _ =    recall(df, gold_annotator_id, predictions_annotator_id)\n",
    "    agreement_precision, _ = precision(df, gold_annotator_id, predictions_annotator_id)\n",
    "    f1_scores =              calculate_f1(agreement_recall, agreement_precision)\n",
    "    \n",
    "    for performance in f1_scores:\n",
    "        performance[\"n_in_gold_standard\"]= len(df[ (df[\"label\"]==performance[\"label\"]) & (df[\"annotator_id\"]==gold_annotator_id) ])\n",
    "        performance[\"n_in_predictions\"]=   len(df[ (df[\"label\"]==performance[\"label\"]) & (df[\"annotator_id\"]==predictions_annotator_id) ])\n",
    "\n",
    "    return pd.DataFrame(f1_scores).sort_values('label', ascending=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f104ac5-3731-477d-bcdf-4a6da847ba2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048a007-812c-4536-bf35-a221efd5a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are all the prompts created for each label. However, as we work only with four labels, only those prompts will be used\n",
    "ita_prompts= {\n",
    "                'dangerous': \"Quale parte del testo è pericolosa (criminalizza, ridicolizza, incita alla violenza, ...) nei confronti di identità vulnerabili (donne, migranti, minoranze etniche, ...)?\",\n",
    "                'derogatory': \"Quale parte del testo è dispregiativa (commenti negativi o denigratori sulla vittima senza un esplicito discorso d'odio, ma in cui la menzione stessa è discriminatoria o offensiva e spesso usa un tono volto a sminuire o screditare la vittima)?\",\n",
    "            \n",
    "                'entity': \"Quale entità nominata è menzionata nella frase?\",\n",
    "                'entity - person': \"¿Quale persona è menzionata nella frase?\",\n",
    "                'entity - group': \"Quale gruppo è menzionato nella frase?\",\n",
    "                'entity - organization': \"Quale organizzazione è menzionata nella frase?\",\n",
    "                'entity - location': \"Quale luogo è menzionato nella frase?\",\n",
    "                'entity - other': \"Quale entità che non sia una persona, un gruppo, un'organizzazione o un luogo è menzionata nella frase?\",\n",
    "                \n",
    "                'vulnerable entity': \"Quale persona, gruppo, organizzazione, luogo, ecc. vulnerabile ai discorsi d'odio è menzionato nella frase?\",\n",
    "                'vulnerable group - migrant': \"Quale parte del testo parla di migranti?\",\n",
    "                'vulnerable group - ethnic minority': \"Quale parte del testo parla di minoranze etniche?\",\n",
    "                'vulnerable group - religious minority': \"Quale parte del testo parla di minoranze religiose?\",\n",
    "                'vulnerable group - women': \"Quale parte del testo parla di donne?\",\n",
    "                'vulnerable group - lgbtq+ community': \"Quale parte del testo parla di persone LGBTQ+\",\n",
    "                'vulnerable group - other': \"Quale parte del testo parla di gruppi vulnerabili che non siano migranti, minoranze etniche o religiose, donne, comunità LGBTQ+?\",\n",
    "                'vulnerable identity': \"Quale identità vulnerabile ai discorsi d'odio è menzionata nella frase?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa692faa-6266-4261-812e-df4a58dcc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset adaptation. In the paper we reduce the work to four label categories: dangerous, derogatory, vulnerable identities and named entities\n",
    "ita_gold= ita_gold[ita_gold[\"label\"]!='vulnerable entity'] # Vulnerable entities are out of scope\n",
    "ita_gold[\"label\"]= ita_gold[\"label\"].map(category_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd7952-f68e-4022-a842-9dbe78957d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ita= [\"gsarti/it5-base\", \"morenolq/bart-it\"] # Models that will be used\n",
    "models_results_ita= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda92b68-ed25-4791-a5e3-90736ff2a46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for qa_italian_model in models_ita: \n",
    "    model= pipeline(\"question-answering\", qa_italian_model, device=device)\n",
    "    r_z_s_ita_df= experiment_restrictive_zero_shot( ita_gold, model=model, model_name=qa_italian_model, prompts=ita_prompts) # Restrictive zero shot experiment\n",
    "    nr_z_s_ita_df= experiment_non_restrictive_zero_shot(ita_gold, model=model, model_name=qa_italian_model, prompts=ita_prompts) # Non restrictive zero shot experiment\n",
    "    ita_df= pd.concat([ita_gold, r_z_s_ita_df, nr_z_s_ita_df])\n",
    "\n",
    "    results_nr_z_s= eval_experiments(ita_df, qa_italian_model, task_name=\"non-restrictive-zero-shot\", gold_annotator_id=\"gold_standard\")\n",
    "    results_nr_z_s= results_nr_z_s[results_nr_z_s[\"label\"].isin(improtant_labels)]\n",
    "\n",
    "    results_r_z_s= eval_experiments(ita_df, qa_italian_model, task_name=\"restrictive-zero-shot\", gold_annotator_id=\"gold_standard\")\n",
    "    results_r_z_s= results_r_z_s[results_r_z_s[\"label\"].isin(improtant_labels)]\n",
    "\n",
    "    models_results_ita[qa_italian_model]= {\"non-restrictive-zero-shot\": results_nr_z_s, \"restrictive-zero-shot\":results_r_z_s}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237e85f-3842-4273-a323-8306d271b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ita_df.to_csv(\"Data/annotations_ita.csv\", index=False) # you can save the execution to a csv\n",
    "ita_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee0895-06db-4d16-b7d8-038205137ef5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c709e0-de08-4002-a988-b76a6d44072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are all the prompts created for each label. However, as we work only with four labels, only those prompts will be used\n",
    "\n",
    "spa_prompts= {\n",
    "                'dangerous': \"¿Qué parte del texto es peligroso (criminaliza, ridiculiza, incita a la violencia, ...) contra identidades vulnerables (mujeres, migrantes, minorías étnicas, ...)?\",\n",
    "                'derogatory': \"¿Qué parte del texto es derogativo (comentarios negativos o despectivos sobre la víctima sin incitación explícita al odio, pero la mención en sí es discriminatoria u ofensiva, y a menudo emplea un tono destinado a menospreciar o desacreditar a la víctima)?\",\n",
    "\n",
    "                'entity': \"¿Qué entidad nombrada se menciona en la frase?\",\n",
    "                'entity - person': \"¿Qué persona se menciona en la frase?\",\n",
    "                'entity - group': \"¿Qué grupo se menciona en la frase?\",\n",
    "                'entity - organization': \"¿Qué organización se menciona en la frase?\",\n",
    "                'entity - location': \"¿Qué lugar se menciona en la frase?\",\n",
    "                'entity - other': \"¿Qué entidad que no es una persona, grupo, organización ni lugar se menciona en la frase?\",\n",
    "                'vulnerable entity': \"¿Qué persona, grupo, organización, lugar, etc. vulnerable al discurso de odio se menciona en la frase?\",\n",
    "\n",
    "                'vulnerable group - migrant': \"¿Qué parte del texto habla sobre los migrantes?\",\n",
    "                'vulnerable group - ethnic minority': \"¿Qué parte del texto habla sobre minorías étnicas?\",\n",
    "                'vulnerable group - religious minority': \"¿Qué parte del texto habla sobre minorías religiosas?\",\n",
    "                'vulnerable group - women': \"¿Qué parte del texto habla sobre las mujeres?\",\n",
    "                'vulnerable group - lgbtq+ community': \"¿Qué parte del texto habla sobre el colectivo LGBTQ+?\",\n",
    "                'vulnerable group - other': \"¿Qué parte del texto habla sobre algún grupo vulnerable que no sean migrantes, minorías étnicas o religiosas, mujeres, colectivo LGBTQ+?\",\n",
    "                'vulnerable identity': \"¿Qué identidad vulnerable al discurso de odio se menciona en la frase?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7698c8-2f8d-4a50-aad3-711dddfcf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset adaptation. In the paper we reduce the work to four label categories: dangerous, derogatory, vulnerable identities and named entities\n",
    "spa_gold= spa_gold[spa_gold[\"label\"]!='vulnerable entity'] # Vulnerable entities are out of scope\n",
    "spa_gold[\"label\"]= spa_gold[\"label\"].map(category_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d03acd-0c6a-4e7f-a10e-61c8402b60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_spa= [\"vgaraujov/t5-base-spanish\", \"vgaraujov/bart-base-spanish\"] # Models that will be used\n",
    "models_results_spa= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af819d8a-e5de-4e25-a45d-2ac2b48deee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for qa_spanish_model in models_spa: \n",
    "    model= pipeline(\"question-answering\", qa_spanish_model, device=device)\n",
    "    r_z_s_spa_df= experiment_restrictive_zero_shot(     spa_gold, model=model, model_name=qa_spanish_model, prompts=spa_prompts) # Restrictive Zero shot experiment\n",
    "    nr_z_s_spa_df= experiment_non_restrictive_zero_shot(spa_gold, model=model, model_name=qa_spanish_model, prompts=spa_prompts) # Non Restrictive Zero shot experiment\n",
    "    spa_df= pd.concat([spa_gold, r_z_s_spa_df, nr_z_s_spa_df])\n",
    "    \n",
    "    results_nr_z_s= eval_experiments(spa_df, qa_spanish_model, task_name=\"non-restrictive-zero-shot\", gold_annotator_id=\"gold_standard\")\n",
    "    results_nr_z_s= results_nr_z_s[results_nr_z_s[\"label\"].isin(improtant_labels)]\n",
    "    \n",
    "    results_r_z_s= eval_experiments(spa_df, qa_spanish_model, task_name=\"restrictive-zero-shot\", gold_annotator_id=\"gold_standard\")\n",
    "    results_r_z_s= results_r_z_s[results_r_z_s[\"label\"].isin(improtant_labels)]\n",
    "\n",
    "    models_results_spa[qa_spanish_model]= {\"non-restrictive-zero-shot\": results_nr_z_s, \"restrictive-zero-shot\":results_r_z_s}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388f2c0-5ff1-4c1c-b755-641d9ca9b0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spa_df.to_csv(\"Data/annotations_spa.csv\", index=False) # you can save the execution to a csv\n",
    "spa_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02766007-ff96-4d00-84c6-ec03d9030b12",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfa911-192c-43df-9a8e-a1e657d2a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Restrictive Zero-shot T5\n",
    "nr_z_s_t5= pd.DataFrame({\n",
    "    \"labels\": models_results_ita[\"gsarti/it5-base\"][\"non-restrictive-zero-shot\"][\"label\"],\n",
    "    \"spanish\": models_results_spa[\"vgaraujov/t5-base-spanish\"][\"non-restrictive-zero-shot\"][\"f1_score\"].round(2),\n",
    "    \"italian\": models_results_ita[\"gsarti/it5-base\"][\"non-restrictive-zero-shot\"][\"f1_score\"].round(2)\n",
    "})\n",
    "print(\"Non-Restrictive Zero-Shot Experiments With T5 Model\")\n",
    "print(nr_z_s_t5.to_latex(index=False)) # Paper table values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2bf89-3164-49dd-ae3c-b760ee47777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Restrictive Zero-shot BART\n",
    "nr_z_s_bart= pd.DataFrame({\n",
    "    \"labels\": models_results_ita[\"morenolq/bart-it\"][\"non-restrictive-zero-shot\"][\"label\"],\n",
    "    \"spanish\": models_results_spa[\"vgaraujov/bart-base-spanish\"][\"non-restrictive-zero-shot\"][\"f1_score\"].round(2),\n",
    "    \"italian\": models_results_ita[\"morenolq/bart-it\"][\"non-restrictive-zero-shot\"][\"f1_score\"].round(2)\n",
    "})\n",
    "print(\"Non-Restrictive Zero-Shot Experiments With BART Model\")\n",
    "print(nr_z_s_bart.to_latex(index=False)) # Paper table values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97301e3-1e20-41bc-bd5b-e82d70ba8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrictive Zero-shot T5\n",
    "r_z_s_t5= pd.DataFrame({\n",
    "    \"labels\": models_results_ita[\"gsarti/it5-base\"][\"restrictive-zero-shot\"][\"label\"],\n",
    "    \"spanish\": models_results_spa[\"vgaraujov/t5-base-spanish\"][\"restrictive-zero-shot\"][\"f1_score\"].round(2),\n",
    "    \"italian\": models_results_ita[\"gsarti/it5-base\"][\"restrictive-zero-shot\"][\"f1_score\"].round(2)\n",
    "})\n",
    "print(\"Restrictive Zero-Shot Experiments With T5 Model\")\n",
    "print(r_z_s_t5.to_latex(index=False)) # Paper table values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1e7da-5948-49b6-a664-3b9334bfae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrictive Zero-shot BART\n",
    "r_z_s_bart= pd.DataFrame({\n",
    "    \"labels\": models_results_ita[\"morenolq/bart-it\"][\"restrictive-zero-shot\"][\"label\"],\n",
    "    \"spanish\": models_results_spa[\"vgaraujov/bart-base-spanish\"][\"restrictive-zero-shot\"][\"f1_score\"].round(2),\n",
    "    \"italian\": models_results_ita[\"morenolq/bart-it\"][\"restrictive-zero-shot\"][\"f1_score\"].round(2)\n",
    "})\n",
    "print(\"Restrictive Zero-Shot Experiments With BART Model\")\n",
    "print(r_z_s_bart.to_latex(index=False)) # Paper table values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
